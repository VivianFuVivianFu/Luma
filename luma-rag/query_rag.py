import os
from dotenv import load_dotenv
from together import Together
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from rag_paths import faiss_path_str

# --- é…ç½® ---
# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# åˆå§‹åŒ– Together AI å®¢æˆ·ç«¯
# å®ƒä¼šè‡ªåŠ¨ä»ç¯å¢ƒå˜é‡ä¸­è¯»å– TOGETHER_API_KEY
try:
    client = Together()
except Exception as e:
    print(f"âŒ åˆå§‹åŒ– Together å®¢æˆ·ç«¯å¤±è´¥: {e}")
    print("ğŸ’¡ è¯·ç¡®ä¿ä½ çš„ .env æ–‡ä»¶ä¸­åŒ…å«äº†æ­£ç¡®çš„ TOGETHER_API_KEYã€‚")
    exit()

# å®šä¹‰å‘é‡æ•°æ®åº“å’ŒåµŒå…¥æ¨¡å‹çš„è·¯å¾„
VECTOR_STORE_PATH = faiss_path_str()
EMBEDDING_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"

# --- æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---

def search_relevant_docs(query_text: str) -> list[str]:
    """
    ä»æœ¬åœ° FAISS å‘é‡æ•°æ®åº“ä¸­æœç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„æ–‡æ¡£ã€‚
    """
    try:
        # ä½¿ç”¨ HuggingFace çš„æ¨¡å‹åœ¨æœ¬åœ°è¿›è¡Œæ–‡æœ¬åµŒå…¥ï¼Œæ— éœ€é¢å¤–API Key
        embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
        
        # åŠ è½½æœ¬åœ°çš„ FAISS ç´¢å¼•
        print("ğŸ” æ­£åœ¨åŠ è½½çŸ¥è¯†åº“...")
        db = FAISS.load_local(faiss_path_str(), embeddings, allow_dangerous_deserialization=True)
        
        # æ‰§è¡Œç›¸ä¼¼æ€§æœç´¢ï¼Œè¿”å›æœ€ç›¸å…³çš„3ä¸ªæ–‡æ¡£
        print(f"ğŸ“š æ­£åœ¨ä¸º â€œ{query_text}â€ æ£€ç´¢ç›¸å…³ä¿¡æ¯...")
        retriever = db.as_retriever(search_kwargs={"k": 3})
        relevant_docs = retriever.invoke(query_text)
        
        # æå–æ–‡æ¡£å†…å®¹
        return [doc.page_content for doc in relevant_docs]

    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: åœ¨ '{VECTOR_STORE_PATH}' ç›®å½•ä¸‹æ‰¾ä¸åˆ°å‘é‡æ•°æ®åº“ã€‚")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ `rag_pipeline.py` æˆ–ç±»ä¼¼çš„è„šæœ¬æ¥æ„å»ºçŸ¥è¯†åº“ã€‚")
        return []
    except Exception as e:
        print(f"âŒ æ–‡æ¡£æ£€ç´¢æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return []

def build_prompt_with_context(query_text: str, context_docs: list[str]) -> str:
    """
    å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£å’Œç”¨æˆ·é—®é¢˜æ‹¼æ¥æˆä¸€ä¸ªå®Œæ•´çš„æç¤ºã€‚
    """
    if not context_docs:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œåªè¿”å›ç”¨æˆ·åŸå§‹é—®é¢˜
        return query_text

    # å°†æ–‡æ¡£å†…å®¹æ ¼å¼åŒ–
    context = "\n\n---\n\n".join(context_docs)
    
    # æ„å»ºæœ€ç»ˆçš„æç¤ºæ¨¡æ¿
    prompt = f"""
è¯·æ ¹æ®ä»¥ä¸‹èƒŒæ™¯çŸ¥è¯†æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœèƒŒæ™¯çŸ¥è¯†ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ ¹æ®ä½ çš„ç†è§£å›ç­”ï¼Œå¹¶ä¿æŒ Luma çš„äººæ ¼è®¾å®šã€‚

[èƒŒæ™¯çŸ¥è¯†]
{context}

[ç”¨æˆ·é—®é¢˜]
{query_text}
"""
    return prompt.strip()

def ask_llama(prompt: str) -> str:
    """
    ä½¿ç”¨ Together.ai çš„ LLaMA 3 æ¨¡å‹ç”Ÿæˆå›ç­”ã€‚
    """
    system_prompt = "You are Luma, a warm, trauma-informed AI companion. You are not a therapist. Your role is to provide gentle, supportive, and reflective responses based on the provided context and your general knowledge. Speak calmly and with empathy."
    
    try:
        response = client.chat.completions.create(
            model="meta-llama/Llama-3-8b-chat",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=512
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"âŒ è°ƒç”¨ LLaMA æ¨¡å‹æ—¶å‡ºé”™: {e}"

def main():
    """
    ä¸»è¿è¡Œå¾ªç¯ï¼Œå¤„ç†ç”¨æˆ·è¾“å…¥å’ŒAIå›å¤ã€‚
    """
    print("="*50)
    print("ğŸ’¬ Luma (RAG Version) is ready.")
    print("   - Type your message and press Enter.")
    print("   - Type 'exit' or 'quit' to end the session.")
    print("="*50)

    while True:
        user_input = input("ğŸ‘¤ You: ")
        if user_input.lower() in ["exit", "quit"]:
            print("âœ¨ Hope this was a supportive moment. Goodbye!")
            break

        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        docs = search_relevant_docs(user_input)
        
        # 2. æ„å»ºå¸¦ä¸Šä¸‹æ–‡çš„æç¤º
        prompt_for_llama = build_prompt_with_context(user_input, docs)
        
        # 3. è·å– LLaMA çš„å›ç­”
        print("ğŸ§  Luma is thinking...")
        answer = ask_llama(prompt_for_llama)
        
        # 4. æ‰“å°å›ç­”
        print(f"\nğŸ’¡ Luma: {answer}\n")

if __name__ == "__main__":
    main()
