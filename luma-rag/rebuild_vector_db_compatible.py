# rebuild_vector_db_compatible.py
# å…¼å®¹ç‰ˆæœ¬çš„å‘é‡æ•°æ®åº“é‡å»º

import os
from dotenv import load_dotenv
from rag_paths import faiss_path_str

print("ğŸ”„ é‡æ–°æ„å»ºå‘é‡æ•°æ®åº“ï¼ˆå…¼å®¹ç‰ˆæœ¬ï¼‰...")
print("="*50)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

try:
    # ä½¿ç”¨æ–°çš„å…¼å®¹æ€§å¯¼å…¥æ–¹æ¡ˆ
    print("ğŸ“¦ å¯¼å…¥ä¾èµ–åŒ…...")
    
    from langchain_community.document_loaders import TextLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import FAISS
    
    # ğŸ”§ ä½¿ç”¨å…¼å®¹æ€§åµŒå…¥å·¥å…·
    from embedding_utils import get_openai_embeddings
    print("âœ… å¯¼å…¥æˆåŠŸ")

    # è®¾ç½®è·¯å¾„
    DOCS_PATH = "docs"
    VECTOR_DB_PATH = faiss_path_str()

    print(f"ğŸ“ æ–‡æ¡£è·¯å¾„: {DOCS_PATH}")
    print(f"ğŸ“ å‘é‡æ•°æ®åº“è·¯å¾„: {VECTOR_DB_PATH}")

    # æ£€æŸ¥æ–‡æ¡£ç›®å½•
    if not os.path.exists(DOCS_PATH):
        print(f"âŒ æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {DOCS_PATH}")
        exit(1)

    # 1. åŠ è½½æ‰€æœ‰æ–‡æœ¬æ–‡æ¡£
    print("\nğŸ“š åŠ è½½æ–‡æ¡£...")
    all_docs = []
    doc_count = 0
    
    for file_name in os.listdir(DOCS_PATH):
        if file_name.endswith(".txt"):
            file_path = os.path.join(DOCS_PATH, file_name)
            try:
                loader = TextLoader(file_path, encoding="utf-8")
                docs = loader.load()
                all_docs.extend(docs)
                doc_count += 1
                print(f"  âœ… {file_name}")
            except Exception as e:
                print(f"  âŒ {file_name}: {e}")

    print(f"ğŸ“– æ€»å…±åŠ è½½äº† {doc_count} ä¸ªæ–‡æ¡£ï¼Œ{len(all_docs)} ä¸ªæ–‡æ¡£å¯¹è±¡")

    if not all_docs:
        print("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ–‡æ¡£")
        exit(1)

    # 2. åˆ†å‰²æ–‡æ¡£
    print("\nâœ‚ï¸ åˆ†å‰²æ–‡æ¡£...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500, 
        chunk_overlap=50
    )
    docs_split = text_splitter.split_documents(all_docs)
    print(f"ğŸ“ åˆ†å‰²åå¾—åˆ° {len(docs_split)} ä¸ªæ–‡æ¡£å—")

    # 3. åˆ›å»ºåµŒå…¥ï¼ˆä½¿ç”¨å…¼å®¹æ€§å·¥å…·ï¼‰
    print("\nğŸ”— åˆ›å»ºåµŒå…¥...")
    try:
        embeddings = get_openai_embeddings()
        print("âœ… Embeddings åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ Embeddings åˆ›å»ºå¤±è´¥: {e}")
        print("ğŸ’¡ è¿™å¯èƒ½æ˜¯ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜")
        exit(1)

    # 4. æ„å»ºå‘é‡æ•°æ®åº“
    print("\nğŸ”¨ æ„å»ºå‘é‡æ•°æ®åº“...")
    try:
        db = FAISS.from_documents(docs_split, embeddings)
        print("âœ… å‘é‡æ•°æ®åº“æ„å»ºæˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ å‘é‡æ•°æ®åº“æ„å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        exit(1)

    # 5. ä¿å­˜å‘é‡æ•°æ®åº“
    print("\nğŸ’¾ ä¿å­˜å‘é‡æ•°æ®åº“...")
    try:
        # å¦‚æœç›®å½•å­˜åœ¨ï¼Œå…ˆåˆ é™¤
        if os.path.exists(VECTOR_DB_PATH):
            import shutil
            shutil.rmtree(VECTOR_DB_PATH)
            print("ğŸ—‘ï¸ åˆ é™¤æ—§çš„å‘é‡æ•°æ®åº“")
        
        db.save_local(faiss_path_str())
        print("âœ… å‘é‡æ•°æ®åº“ä¿å­˜æˆåŠŸ")
        
        # éªŒè¯ä¿å­˜
        saved_files = os.listdir(VECTOR_DB_PATH)
        print(f"ğŸ“ ä¿å­˜çš„æ–‡ä»¶: {saved_files}")
        
        # æµ‹è¯•åŠ è½½
        print("\nğŸ§ª æµ‹è¯•é‡æ–°åŠ è½½...")
        db_test = FAISS.load_local(faiss_path_str(), embeddings, allow_dangerous_deserialization=True)
        test_results = db_test.similarity_search("attachment", k=2)
        print(f"âœ… æµ‹è¯•æˆåŠŸï¼Œæ‰¾åˆ° {len(test_results)} ä¸ªç»“æœ")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜/æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        exit(1)

    print("\n" + "="*50)
    print("ğŸ‰ å‘é‡æ•°æ®åº“é‡å»ºå®Œæˆï¼")
    print("âœ… ç°åœ¨å¯ä»¥è¿è¡ŒæŸ¥è¯¢ç³»ç»Ÿäº†")
    
except Exception as e:
    print(f"âŒ é‡å»ºè¿‡ç¨‹å‡ºé”™: {e}")
    import traceback
    traceback.print_exc()
    
    print("\nğŸ’¡ å»ºè®®æ‰‹åŠ¨æ“ä½œæ­¥éª¤ï¼š")
    print("1. è¿è¡Œ: pip install --upgrade langchain langchain-openai openai")
    print("2. é‡æ–°è¿è¡Œ: python rag_pipeline.py")
    print("3. æˆ–è€…è¿è¡Œ: python rebuild_vector_db_compatible.py")
