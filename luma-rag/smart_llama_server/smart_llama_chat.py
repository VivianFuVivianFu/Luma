#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Smart LLaMA Chat Server - LLaMA 3 70B First, RAG When Needed
Port: 5001
"""

from flask import Flask, jsonify, request
import os
import requests
import json
from dotenv import load_dotenv

# Load environment variables
# Try to load .env from current directory first, then parent directory
load_dotenv(override=True)
parent_env = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), '.env')
if os.path.exists(parent_env):
    load_dotenv(parent_env, override=True)
    print(f"ğŸ“ Environment loaded from: {parent_env}")

app = Flask(__name__)

# API Configuration
TOGETHER_API_KEY = os.getenv("TOGETHER_API_KEY", "")
TOGETHER_API_URL = "https://api.together.xyz/v1/chat/completions"

# Try to import RAG functions (optional)
try:
    import sys
    import os
    # Add parent directory to path to access query_rag module
    parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    sys.path.insert(0, parent_dir)
    
    from query_rag import search_relevant_docs, build_prompt_with_context
    RAG_AVAILABLE = True
    print("âœ… RAG module loaded - will be used for complex queries")
    print(f"ğŸ“ RAG module loaded from: {parent_dir}")
except ImportError as e:
    RAG_AVAILABLE = False
    print(f"âš ï¸  RAG module not available - using LLaMA only")
    print(f"ğŸ“ Import error: {str(e)}")

# Global conversation memory
conversation_memory = []

# Keywords that trigger RAG usage
RAG_TRIGGER_KEYWORDS = [
    "å…·ä½“çš„", "è¯¦ç»†çš„", "ä¸“ä¸šçš„", "æŠ€æœ¯ç»†èŠ‚", "æ–‡æ¡£", "èµ„æ–™", 
    "reference", "documentation", "specific", "detailed", "technical",
    "how to implement", "code example", "step by step", "tutorial",
    "ä»€ä¹ˆæ˜¯", "å¦‚ä½•å®ç°", "ä»£ç ç¤ºä¾‹", "æ•™ç¨‹", "æ­¥éª¤"
]

def should_use_rag(message):
    """
    åˆ¤æ–­æ˜¯å¦éœ€è¦ä½¿ç”¨RAGæ£€ç´¢
    """
    message_lower = message.lower()
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«è§¦å‘è¯
    for keyword in RAG_TRIGGER_KEYWORDS:
        if keyword in message_lower:
            return True
    
    # æ£€æŸ¥æ¶ˆæ¯é•¿åº¦ - å¾ˆé•¿çš„é—®é¢˜å¯èƒ½éœ€è¦è¯¦ç»†å›ç­”
    if len(message) > 100:
        return True
        
    # æ£€æŸ¥æ˜¯å¦åŒ…å«é—®å·ä¸”è¾ƒé•¿
    if "?" in message and len(message) > 50:
        return True
    if "ï¼Ÿ" in message and len(message) > 50:
        return True
        
    return False

def call_llama_api(messages, use_rag_context=False):
    """
    è°ƒç”¨ LLaMA 3 70B API
    """
    try:
        headers = {
            "Authorization": f"Bearer {TOGETHER_API_KEY}",
            "Content-Type": "application/json"
        }
        
        # æ ¹æ®æ˜¯å¦ä½¿ç”¨RAGè°ƒæ•´ç³»ç»Ÿæç¤º
        if use_rag_context:
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œèƒ½å¤ŸåŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ç»™å‡ºå‡†ç¡®ã€è¯¦ç»†çš„å›ç­”ã€‚
è¯·æ ¹æ®ç”¨æˆ·é—®é¢˜å’Œç›¸å…³æ–‡æ¡£å†…å®¹ï¼Œæä¾›ä¸“ä¸šè€Œæœ‰ç”¨çš„å›å¤ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®è¯´æ˜å¹¶æä¾›ä½ çš„ä¸€èˆ¬æ€§å»ºè®®ã€‚
è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­è¨€è‡ªç„¶å‹å¥½ã€‚"""
        else:
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªå‹å¥½ã€æ™ºèƒ½çš„AIåŠ©æ‰‹ã€‚è¯·ç”¨è‡ªç„¶ã€æœ‰è¶£çš„æ–¹å¼ä¸ç”¨æˆ·å¯¹è¯ã€‚
- ä¿æŒå¯¹è¯è½»æ¾æ„‰å¿«
- å›ç­”è¦ç®€æ´æ˜äº†ï¼Œä¸è¦è¿‡äºå†—é•¿
- ç”¨ä¸­æ–‡å›ç­”
- å¦‚æœé‡åˆ°ä½ ä¸ç¡®å®šçš„ä¸“ä¸šé—®é¢˜ï¼Œå¯ä»¥å»ºè®®ç”¨æˆ·æä¾›æ›´å¤šç»†èŠ‚"""

        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        api_messages = [{"role": "system", "content": system_prompt}]
        api_messages.extend(messages)
        
        payload = {
            "model": "meta-llama/Llama-3-70b-chat-hf",
            "messages": api_messages,
            "max_tokens": 1000,
            "temperature": 0.7,
            "top_p": 0.9,
            "repetition_penalty": 1.1
        }
        
        print(f"[DEBUG] Calling LLaMA API, use_rag_context: {use_rag_context}")
        
        response = requests.post(
            TOGETHER_API_URL,
            headers=headers,
            json=payload,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            return result["choices"][0]["message"]["content"]
        else:
            print(f"[ERROR] API call failed: {response.status_code}, {response.text}")
            return f"æŠ±æ­‰ï¼ŒAPIè°ƒç”¨å¤±è´¥ã€‚çŠ¶æ€ç : {response.status_code}"
            
    except Exception as e:
        print(f"[ERROR] Exception in API call: {str(e)}")
        return f"æŠ±æ­‰ï¼Œè°ƒç”¨è¯­è¨€æ¨¡å‹æ—¶å‡ºç°é”™è¯¯: {str(e)}"

@app.route("/test")
def test():
    return jsonify({
        "status": "OK", 
        "message": "Smart LLaMA Chat Server on 5001",
        "llama_configured": bool(TOGETHER_API_KEY),
        "rag_available": RAG_AVAILABLE
    })

@app.route("/chat", methods=["POST"])
def chat():
    global conversation_memory
    
    try:
        data = request.get_json() or {}
        user_message = data.get("message", "").strip()
        
        if not user_message:
            return jsonify({"error": "è¯·æä¾›æ¶ˆæ¯å†…å®¹"}), 400
        
        if not TOGETHER_API_KEY:
            return jsonify({"error": "Together APIå¯†é’¥æœªé…ç½®"}), 500
        
        print(f"[INFO] User message: {user_message}")
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦ä½¿ç”¨RAG
        needs_rag = should_use_rag(user_message) and RAG_AVAILABLE
        print(f"[INFO] Using RAG: {needs_rag}")
        
        # å‡†å¤‡å¯¹è¯æ¶ˆæ¯
        recent_messages = conversation_memory[-6:] if conversation_memory else []
        messages = recent_messages + [{"role": "user", "content": user_message}]
        
        if needs_rag:
            # ä½¿ç”¨RAGå¢å¼ºå›ç­”
            try:
                relevant_docs = search_relevant_docs(user_message)
                if relevant_docs:
                    # æ„å»ºå¸¦æœ‰ä¸Šä¸‹æ–‡çš„æ¶ˆæ¯
                    context = "\n".join([doc.get("content", "") for doc in relevant_docs[:3]])
                    enhanced_message = f"""åŸºäºä»¥ä¸‹ç›¸å…³ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ï¼š

ç›¸å…³ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{user_message}

è¯·æ ¹æ®ä¸Šè¿°ä¿¡æ¯ç»™å‡ºè¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯´æ˜å¹¶æä¾›ä½ çš„å»ºè®®ã€‚"""
                    
                    messages[-1]["content"] = enhanced_message
                    response = call_llama_api(messages, use_rag_context=True)
                    
                    print(f"[INFO] RAG enhanced response generated")
                    
                    return jsonify({
                        "response": response,
                        "mode": "RAG_Enhanced",
                        "relevant_docs_count": len(relevant_docs)
                    })
                else:
                    print("[INFO] No relevant docs found, using direct LLaMA")
                    
            except Exception as e:
                print(f"[ERROR] RAG failed: {str(e)}, falling back to direct LLaMA")
        
        # ç›´æ¥ä½¿ç”¨LLaMA 3 70B
        response = call_llama_api(messages, use_rag_context=False)
        
        # æ›´æ–°å¯¹è¯å†å²
        conversation_memory.append({"role": "user", "content": user_message})
        conversation_memory.append({"role": "assistant", "content": response})
        
        # ä¿æŒå¯¹è¯å†å²åœ¨åˆç†é•¿åº¦
        if len(conversation_memory) > 20:
            conversation_memory = conversation_memory[-20:]
        
        print(f"[INFO] Direct LLaMA response generated")
        
        return jsonify({
            "response": response,
            "mode": "Direct_LLaMA",
            "conversation_length": len(conversation_memory)
        })
        
    except Exception as e:
        print(f"[ERROR] Chat error: {str(e)}")
        return jsonify({"error": f"èŠå¤©å¤„ç†é”™è¯¯: {str(e)}"}), 500

@app.route("/reset_memory", methods=["POST"])
def reset_memory():
    global conversation_memory
    conversation_memory = []
    print("[INFO] Conversation memory reset")
    return jsonify({"message": "å¯¹è¯è®°å¿†å·²é‡ç½®"})

@app.route("/debug", methods=["GET"])
def debug():
    return jsonify({
        "together_api_configured": bool(TOGETHER_API_KEY),
        "rag_available": RAG_AVAILABLE,
        "conversation_length": len(conversation_memory),
        "rag_triggers": RAG_TRIGGER_KEYWORDS[:5],  # æ˜¾ç¤ºéƒ¨åˆ†è§¦å‘è¯
        "last_messages": conversation_memory[-4:] if conversation_memory else []
    })

@app.route("/")
def home():
    return jsonify({
        "status": "Smart LLaMA Chat Server",
        "port": 5001,
        "strategy": "LLaMA-first, RAG-when-needed",
        "endpoints": ["/test", "/chat", "/debug", "/reset_memory"]
    })

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ¤– Smart LLaMA Chat Server")
    print("=" * 60)
    print("ğŸ§  Primary: LLaMA 3 70B Instruct")
    print("ğŸ” Secondary: RAG for complex queries")
    print("ğŸ“ Server: http://localhost:5001")
    print("=" * 60)
    print(f"âœ… Together API: {'Configured' if TOGETHER_API_KEY else 'âŒ Not configured'}")
    print(f"âœ… RAG System: {'Available' if RAG_AVAILABLE else 'âŒ Not available'}")
    print("=" * 60)
    
    app.run(host="0.0.0.0", port=5001, debug=True)
