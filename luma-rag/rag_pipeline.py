# rag_pipeline.py
# æ”¹è¿›ç‰ˆæœ¬ï¼Œå¢åŠ é”™è¯¯å¤„ç†

import os
import dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
dotenv.load_dotenv()

def main():
    print("ğŸš€ å¼€å§‹æ„å»ºçŸ¥è¯†åº“...")
    print("="*40)

    # æ£€æŸ¥ API Key
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ æœªæ‰¾åˆ° OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        print("ğŸ’¡ è¯·æ£€æŸ¥ .env æ–‡ä»¶")
        return False

    try:
        # å¯¼å…¥ä¾èµ–
        print("ğŸ“¦ å¯¼å…¥ä¾èµ–åŒ…...")
        from langchain_community.document_loaders import TextLoader
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        from langchain_community.vectorstores import FAISS
        
        # å°è¯•å¯¼å…¥ OpenAI Embeddings
        try:
            from langchain_openai import OpenAIEmbeddings
            print("âœ… ä½¿ç”¨ langchain_openai")
        except ImportError:
            print("âš ï¸ langchain_openai ä¸å¯ç”¨ï¼Œå°è¯•æ—§ç‰ˆæœ¬...")
            from langchain.embeddings.openai import OpenAIEmbeddings
            print("âœ… ä½¿ç”¨ langchain.embeddings.openai")

        # è®¾ç½®è·¯å¾„
        DOCS_PATH = "docs"
        VECTOR_DB_PATH = "vector_store"

        print(f"ğŸ“ æ–‡æ¡£è·¯å¾„: {DOCS_PATH}")
        print(f"ğŸ“ å‘é‡åº“è·¯å¾„: {VECTOR_DB_PATH}")

        # æ£€æŸ¥æ–‡æ¡£ç›®å½•
        if not os.path.exists(DOCS_PATH):
            print(f"âŒ æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {DOCS_PATH}")
            return False

        # 1ï¸âƒ£ åŠ è½½æ‰€æœ‰æ–‡æœ¬
        print("\nğŸ“š åŠ è½½æ–‡æ¡£...")
        all_docs = []
        doc_count = 0
        
        txt_files = [f for f in os.listdir(DOCS_PATH) if f.endswith(".txt")]
        if not txt_files:
            print(f"âŒ åœ¨ {DOCS_PATH} ä¸­æœªæ‰¾åˆ° .txt æ–‡ä»¶")
            return False

        for file_name in txt_files:
            try:
                file_path = os.path.join(DOCS_PATH, file_name)
                loader = TextLoader(file_path, encoding="utf-8")
                docs = loader.load()
                all_docs.extend(docs)
                doc_count += 1
                print(f"  âœ… {file_name}")
            except Exception as e:
                print(f"  âŒ {file_name}: {e}")

        print(f"ğŸ“– æˆåŠŸåŠ è½½ {doc_count} ä¸ªæ–‡æ¡£ï¼Œå…± {len(all_docs)} ä¸ªæ–‡æ¡£å¯¹è±¡")

        if not all_docs:
            print("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ–‡æ¡£")
            return False

        # 2ï¸âƒ£ åˆ‡å‰²æ–‡æœ¬ä¸ºå°å—
        print("\nâœ‚ï¸ åˆ†å‰²æ–‡æ¡£...")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500, 
            chunk_overlap=50
        )
        docs_split = text_splitter.split_documents(all_docs)
        print(f"ğŸ“ åˆ†å‰²æˆ {len(docs_split)} ä¸ªæ–‡æ¡£å—")

        # 3ï¸âƒ£ åˆ›å»ºåµŒå…¥
        print("\nğŸ”— åˆ›å»º OpenAI Embeddings...")
        try:
            # å…ˆå°è¯•ä¸ä¼ é€’ä»»ä½•å‚æ•°
            embeddings = OpenAIEmbeddings()
            print("âœ… Embeddings åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ é»˜è®¤æ–¹å¼å¤±è´¥: {e}")
            # å°è¯•æ˜¾å¼ä¼ é€’ API key
            try:
                embeddings = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))
                print("âœ… Embeddings åˆ›å»ºæˆåŠŸï¼ˆæ˜¾å¼ API keyï¼‰")
            except Exception as e2:
                print(f"âŒ Embeddings åˆ›å»ºå¤±è´¥: {e2}")
                return False

        # æµ‹è¯• embeddings
        print("ğŸ§ª æµ‹è¯• Embeddings...")
        try:
            test_result = embeddings.embed_query("test")
            print(f"âœ… æµ‹è¯•æˆåŠŸï¼Œå‘é‡ç»´åº¦: {len(test_result)}")
        except Exception as e:
            print(f"âŒ Embeddings æµ‹è¯•å¤±è´¥: {e}")
            return False

        # 4ï¸âƒ£ æ„å»º FAISS å‘é‡åº“
        print("\nğŸ”¨ æ„å»ºå‘é‡åº“...")
        print("   è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
        try:
            db = FAISS.from_documents(docs_split, embeddings)
            print("âœ… å‘é‡åº“æ„å»ºæˆåŠŸ")
        except Exception as e:
            print(f"âŒ å‘é‡åº“æ„å»ºå¤±è´¥: {e}")
            return False

        # å¿«é€Ÿæµ‹è¯•
        print("ğŸ” æµ‹è¯•å‘é‡åº“...")
        try:
            test_results = db.similarity_search("therapy", k=2)
            print(f"âœ… æµ‹è¯•æˆåŠŸï¼Œæ‰¾åˆ° {len(test_results)} ä¸ªç»“æœ")
        except Exception as e:
            print(f"âš ï¸ æµ‹è¯•å¤±è´¥: {e}")

        # 5ï¸âƒ£ ä¿å­˜åˆ°ç£ç›˜
        print("\nğŸ’¾ ä¿å­˜å‘é‡åº“...")
        try:
            # å¦‚æœç›®å½•å­˜åœ¨ï¼Œå…ˆåˆ é™¤
            if os.path.exists(VECTOR_DB_PATH):
                import shutil
                shutil.rmtree(VECTOR_DB_PATH)
                print("ğŸ—‘ï¸ åˆ é™¤æ—§å‘é‡åº“")

            db.save_local(VECTOR_DB_PATH)
            print("âœ… å‘é‡åº“ä¿å­˜æˆåŠŸ")
            
            # éªŒè¯ä¿å­˜çš„æ–‡ä»¶
            saved_files = os.listdir(VECTOR_DB_PATH)
            print(f"ğŸ“ ä¿å­˜çš„æ–‡ä»¶: {saved_files}")

        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
            return False

        print("\n" + "="*40)
        print("ğŸ‰ çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼")
        print(f"ğŸ“Š ç»Ÿè®¡:")
        print(f"   - æ–‡æ¡£æ•°: {doc_count}")
        print(f"   - æ–‡æ¡£å—: {len(docs_split)}")
        print(f"   - ä¿å­˜ä½ç½®: {VECTOR_DB_PATH}")
        print("\nâœ… ç°åœ¨å¯ä»¥è¿›è¡ŒæŸ¥è¯¢äº†ï¼")
        return True

    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        print("ğŸ’¡ å¯èƒ½éœ€è¦å®‰è£…æˆ–å‡çº§ä¾èµ–åŒ…")
        return False
    except Exception as e:
        print(f"âŒ æ„å»ºè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    if not success:
        print("\nâŒ çŸ¥è¯†åº“æ„å»ºå¤±è´¥")
        print("ğŸ’¡ è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶é‡è¯•")
    else:
        print("\nğŸŠ æ„å»ºæˆåŠŸå®Œæˆï¼")
        
