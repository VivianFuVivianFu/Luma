# query_rag.py
# RAG æŸ¥è¯¢å’Œé—®ç­”è„šæœ¬

import os
from dotenv import load_dotenv
from together import Together
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from rag_paths import faiss_path_str

# âœ… è‡ªåŠ¨åŠ è½½ .env ä¸­çš„ key
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
together_api_key = os.getenv("TOGETHER_API_KEY")

# âœ… åˆå§‹åŒ– Together å®¢æˆ·ç«¯
client = Together(api_key=together_api_key)

# è®¾ç½®è·¯å¾„
VECTOR_DB_PATH = faiss_path_str()

def load_vectorstore():
    """åŠ è½½å·²æ„å»ºçš„å‘é‡æ•°æ®åº“"""
    embeddings = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))
    db = FAISS.load_local(faiss_path_str(), embeddings, allow_dangerous_deserialization=True)
    return db

def search_relevant_docs(question: str, k: int = 5):
    """æœç´¢ç›¸å…³æ–‡æ¡£"""
    try:
        db = load_vectorstore()
        docs = db.similarity_search(question, k=k)
        return [doc.page_content for doc in docs]
    except Exception as e:
        print(f"[âš ï¸ æ–‡æ¡£æœç´¢å¤±è´¥]ï¼š{e}")
        return []

def build_prompt_with_context(question: str, docs: list) -> str:
    """æ„å»ºåŒ…å«ä¸Šä¸‹æ–‡çš„æç¤º"""
    context = "\n\n".join(docs[:3])  # ä½¿ç”¨å‰3ä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£
    
    prompt = f"""ä½œä¸ºLumaï¼Œä¸€ä¸ªæ¸©æš–ã€åˆ›ä¼¤çŸ¥æƒ…çš„AIä¼´ä¾£ï¼Œä¸“é—¨ä»äº‹CPTSDæ²»ç–—ã€å†…åœ¨å®¶åº­ç³»ç»Ÿå’Œç¥ç»å¿ƒç†å­¦ã€‚

åŸºäºä»¥ä¸‹ç›¸å…³æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼š

ç›¸å…³èƒŒæ™¯èµ„æ–™ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·ä»¥Lumaçš„èº«ä»½ï¼Œç”¨æ¸©æš–ã€æ”¯æŒå’Œä¸“ä¸šçš„è¯­è°ƒå›ç­”ã€‚å¦‚æœé—®é¢˜æ¶‰åŠä¸“ä¸šæ²»ç–—å»ºè®®ï¼Œè¯·æé†’ç”¨æˆ·å’¨è¯¢ä¸“ä¸šæ²»ç–—å¸ˆã€‚"""
    
    return prompt

# âœ… åˆ¤æ–­é—®é¢˜æ˜¯å¦å¤æ‚ï¼ˆéœ€RAGï¼‰
def is_complex_question(question: str) -> bool:
    try:
        # ä½¿ç”¨ ChatOpenAI çš„æ–°API
        llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
        
        # åˆ›å»ºåˆ¤æ–­æç¤º
        classification_prompt = f"""åˆ¤æ–­ä»¥ä¸‹é—®é¢˜æ˜¯å¦éœ€è¦ä¸“ä¸šçŸ¥è¯†æˆ–æ–‡æ¡£æ”¯æŒã€‚
        
é—®é¢˜ï¼š{question}

å¦‚æœè¿™æ˜¯å…³äºå¿ƒç†å­¦ã€åˆ›ä¼¤ã€æ²»ç–—ã€åŒ»å­¦æˆ–éœ€è¦ä¸“ä¸šèƒŒæ™¯çŸ¥è¯†çš„é—®é¢˜ï¼Œå›ç­”'complex'ã€‚
å¦‚æœè¿™æ˜¯ç®€å•çš„æ—¥å¸¸å¯¹è¯ã€é—®å€™æˆ–åŸºæœ¬é—®é¢˜ï¼Œå›ç­”'simple'ã€‚

åªå›ç­”'complex'æˆ–'simple'ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        
        response = llm.invoke(classification_prompt)
        classification = response.content.strip().lower()
        return "complex" in classification
    except Exception as e:
        print(f"[âš ï¸ åˆ¤æ–­é—®é¢˜ç±»å‹å¤±è´¥ï¼Œé»˜è®¤complex]ï¼š{e}")
        return True  # é»˜è®¤ä½¿ç”¨RAGä»¥è·å¾—æ›´å¥½çš„å›ç­”

# âœ… ç”¨ Together çš„ LLaMA æ¨¡å‹å›ç­”
def ask_llama(prompt: str) -> str:
    try:
        response = client.chat.completions.create(
            model="meta-llama/Llama-3.2-3B-Instruct-Turbo",  # ä½¿ç”¨å¯ç”¨çš„æ¨¡å‹
            messages=[
                {"role": "system", "content": "You are Luma, a warm, trauma-informed AI companion specializing in CPTSD healing, internal family systems, and neuropsychology."},
                {"role": "user", "content": prompt}
            ]
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"[âŒ LLaMAå›ç­”å¤±è´¥]ï¼š{e}"

# âœ… ä¸»è¿è¡Œé€»è¾‘
def main():
    print("ğŸ’¬ Luma is ready. Type your message (or type 'exit' to quit):\n")
    
    # æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦å­˜åœ¨
    if not os.path.exists(VECTOR_DB_PATH):
        print("âš ï¸ å‘é‡æ•°æ®åº“ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ rag_pipeline.py æ„å»ºçŸ¥è¯†åº“")
        return
    
    while True:
        user_input = input("ğŸ‘¤ You: ")
        if user_input.lower() in ["exit", "quit"]:
            print("ğŸ‘‹ Goodbye! Take care.")
            break

        if is_complex_question(user_input):
            print("ğŸ“š å¤æ‚é—®é¢˜ï¼Œæ‰§è¡Œæ–‡æ¡£æ£€ç´¢ RAG...")
            docs = search_relevant_docs(user_input)
            if docs:
                prompt = build_prompt_with_context(user_input, docs)
            else:
                print("ğŸ“ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œä½¿ç”¨åŸºç¡€å¯¹è¯æ¨¡å¼")
                prompt = user_input
        else:
            print("ğŸ§¸ ç®€å•é—®é¢˜ï¼Œç›´æ¥ä½¿ç”¨ LLaMA å¯¹è¯")
            prompt = user_input

        answer = ask_llama(prompt)
        print(f"\nğŸ§  Luma: {answer}\n")

if __name__ == "__main__":
    main()
