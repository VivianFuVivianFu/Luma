# quick_test.py
# å¿«é€Ÿæµ‹è¯•æ‚¨çš„é—®é¢˜

print("ğŸ” å¿«é€Ÿæµ‹è¯•å¼€å§‹...")

try:
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    import os
    from dotenv import load_dotenv
    from rag_paths import faiss_path_str
    load_dotenv()
    
    openai_key = os.getenv("OPENAI_API_KEY")
    together_key = os.getenv("TOGETHER_API_KEY")
    
    print(f"OpenAI Key: {'âœ… å·²è®¾ç½®' if openai_key else 'âŒ æœªè®¾ç½®'}")
    print(f"Together Key: {'âœ… å·²è®¾ç½®' if together_key else 'âŒ æœªè®¾ç½®'}")
    
    # æ£€æŸ¥ä¾èµ–åŒ…
    try:
        from together import Together
        print("âœ… together åŒ…å·²å®‰è£…")
    except ImportError:
        print("âŒ together åŒ…æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install together")
        
    try:
        from langchain_openai import OpenAIEmbeddings
        print("âœ… langchain_openai åŒ…å·²å®‰è£…")
    except ImportError:
        print("âŒ langchain_openai åŒ…æœªå®‰è£…")
        
    try:
        from langchain_community.vectorstores import FAISS
        print("âœ… langchain_community åŒ…å·²å®‰è£…")
    except ImportError:
        print("âŒ langchain_community åŒ…æœªå®‰è£…")
    
    # æ£€æŸ¥å‘é‡æ•°æ®åº“
    if os.path.exists(faiss_path_str()):
        print("âœ… å‘é‡æ•°æ®åº“å­˜åœ¨")
        
        # å°è¯•åŠ è½½
        embeddings = OpenAIEmbeddings()
        db = FAISS.load_local(faiss_path_str(), embeddings, allow_dangerous_deserialization=True)
        print(f"âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸï¼ŒåŒ…å« {db.index.ntotal} ä¸ªæ–‡æ¡£å—")
        
        # æµ‹è¯•æœç´¢
        question = "ä¸ºä»€ä¹ˆcptsdçš„äººå®¹æ˜“å¾—ç„¦è™‘å‹ä¾æ‹"
        results = db.similarity_search(question, k=3)
        print(f"âœ… æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
        
        if results:
            print(f"ğŸ“– ç¬¬ä¸€ä¸ªæ–‡æ¡£é¢„è§ˆ: {results[0].page_content[:150]}...")
            
    else:
        print("âŒ å‘é‡æ•°æ®åº“ä¸å­˜åœ¨")

except Exception as e:
    print(f"âŒ é”™è¯¯: {e}")
    import traceback
    traceback.print_exc()

print("\nğŸ” æµ‹è¯•å®Œæˆï¼")
