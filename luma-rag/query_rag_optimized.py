# query_rag_optimized.py
# ä¼˜åŒ–å»¶è¿Ÿçš„å¤šè¯­è¨€ RAG ç³»ç»Ÿ

import os
import asyncio
import time
import json
from concurrent.futures import ThreadPoolExecutor
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import dotenv
import re
import hashlib

# åŠ è½½ç¯å¢ƒå˜é‡
dotenv.load_dotenv()

# è®¾ç½®è·¯å¾„
VECTOR_DB_PATH = "vector_store"
CACHE_FILE = "translation_cache.json"

class TranslationCache:
    """ç¿»è¯‘ç¼“å­˜ç³»ç»Ÿ"""
    def __init__(self):
        self.cache = self.load_cache()
    
    def load_cache(self):
        """åŠ è½½ç¼“å­˜"""
        try:
            if os.path.exists(CACHE_FILE):
                with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except:
            pass
        return {}
    
    def save_cache(self):
        """ä¿å­˜ç¼“å­˜"""
        try:
            with open(CACHE_FILE, 'w', encoding='utf-8') as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
        except:
            pass
    
    def get_cache_key(self, text):
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.md5(text.encode()).hexdigest()
    
    def get_translation(self, text, direction):
        """è·å–ç¼“å­˜çš„ç¿»è¯‘"""
        key = f"{direction}_{self.get_cache_key(text)}"
        return self.cache.get(key)
    
    def set_translation(self, text, translation, direction):
        """è®¾ç½®ç¼“å­˜"""
        key = f"{direction}_{self.get_cache_key(text)}"
        self.cache[key] = translation
        self.save_cache()

# å…¨å±€ç¼“å­˜å®ä¾‹
translation_cache = TranslationCache()

def detect_language(text):
    """æ£€æµ‹æ–‡æœ¬è¯­è¨€"""
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    total_chars = len(text.strip())
    
    if total_chars == 0:
        return "en"
    
    chinese_ratio = chinese_chars / total_chars
    return "zh" if chinese_ratio > 0.3 else "en"

def translate_text_fast(text, to_language="en"):
    """å¿«é€Ÿç¿»è¯‘ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    direction = f"zh_to_{to_language}" if to_language == "en" else f"en_to_{to_language}"
    
    # æ£€æŸ¥ç¼“å­˜
    cached = translation_cache.get_translation(text, direction)
    if cached:
        print(f"ğŸš€ ä½¿ç”¨ç¼“å­˜ç¿»è¯‘")
        return cached
    
    # æ²¡æœ‰ç¼“å­˜ï¼Œè¿›è¡Œç¿»è¯‘
    print(f"ğŸ”„ æ­£åœ¨ç¿»è¯‘...")
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
    
    if to_language == "en":
        prompt = f"""Translate to English (preserve psychological terms): {text}"""
    else:
        prompt = f"""Translate to Chinese (preserve psychological terms): {text}"""
    
    start_time = time.time()
    response = llm.invoke(prompt)
    translation = response.content.strip()
    end_time = time.time()
    
    print(f"â±ï¸ ç¿»è¯‘è€—æ—¶: {end_time - start_time:.2f}ç§’")
    
    # ä¿å­˜åˆ°ç¼“å­˜
    translation_cache.set_translation(text, translation, direction)
    
    return translation

def load_vectorstore():
    """åŠ è½½å·²æ„å»ºçš„å‘é‡æ•°æ®åº“"""
    embeddings = OpenAIEmbeddings()
    db = FAISS.load_local(VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)
    return db

def create_qa_chain(db):
    """åˆ›å»ºé—®ç­”é“¾"""
    retriever = db.as_retriever(search_kwargs={"k": 5})
    
    # ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹è¿›è¡Œé—®ç­”
    llm = ChatOpenAI(
        model_name="gpt-3.5-turbo",  # æ›´å¿«çš„æ¨¡å‹
        temperature=0.7
    )
    
    prompt_template = """
    You are a professional mental health assistant. Answer based on the document content.
    Be concise but informative. If information is insufficient, state that clearly.
    
    Document content:
    {context}
    
    Question: {question}
    
    Answer:"""
    
    PROMPT = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "question"]
    )
    
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": PROMPT},
        return_source_documents=True
    )
    
    return qa_chain

def ask_question_optimized(qa_chain, question):
    """ä¼˜åŒ–çš„é—®ç­”å‡½æ•°"""
    start_time = time.time()
    
    user_language = detect_language(question)
    print(f"\nğŸŒ æ£€æµ‹è¯­è¨€: {'ä¸­æ–‡' if user_language == 'zh' else 'è‹±æ–‡'}")
    
    # é˜¶æ®µ1: ç¿»è¯‘é—®é¢˜ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if user_language == "zh":
        translation_start = time.time()
        english_question = translate_text_fast(question, "en")
        translation_end = time.time()
        print(f"ğŸ“ è‹±æ–‡é—®é¢˜: {english_question}")
        print(f"â±ï¸ é—®é¢˜ç¿»è¯‘è€—æ—¶: {translation_end - translation_start:.2f}ç§’")
        query_question = english_question
    else:
        query_question = question
    
    # é˜¶æ®µ2: æ£€ç´¢å’Œç”Ÿæˆç­”æ¡ˆ
    retrieval_start = time.time()
    result = qa_chain({"query": query_question})
    english_answer = result["result"]
    retrieval_end = time.time()
    print(f"â±ï¸ æ£€ç´¢+ç”Ÿæˆè€—æ—¶: {retrieval_end - retrieval_start:.2f}ç§’")
    
    # é˜¶æ®µ3: ç¿»è¯‘ç­”æ¡ˆï¼ˆå¦‚æœéœ€è¦ï¼‰
    if user_language == "zh":
        answer_translation_start = time.time()
        chinese_answer = translate_text_fast(english_answer, "zh")
        answer_translation_end = time.time()
        print(f"â±ï¸ ç­”æ¡ˆç¿»è¯‘è€—æ—¶: {answer_translation_end - answer_translation_start:.2f}ç§’")
        
        print(f"\nğŸ¤– ä¸­æ–‡å›ç­”ï¼š")
        print(chinese_answer)
        
        # åªåœ¨éœ€è¦æ—¶æ˜¾ç¤ºè‹±æ–‡åŸç­”æ¡ˆ
        show_original = input("\nğŸ” æ˜¯å¦æ˜¾ç¤ºè‹±æ–‡åŸç­”æ¡ˆ? (y/n): ").lower() == 'y'
        if show_original:
            print(f"\nğŸ“„ è‹±æ–‡åŸç­”æ¡ˆï¼š")
            print(english_answer)
    else:
        print(f"\nğŸ¤– Answer:")
        print(english_answer)
    
    # æ˜¾ç¤ºæ¥æº
    print(f"\nğŸ“š ç›¸å…³æ–‡æ¡£æ¥æºï¼š")
    for i, doc in enumerate(result["source_documents"], 1):
        source = doc.metadata.get("source", "æœªçŸ¥æ¥æº")
        print(f"  {i}. {os.path.basename(source)}")
    
    end_time = time.time()
    total_time = end_time - start_time
    print(f"\nâ±ï¸ æ€»è€—æ—¶: {total_time:.2f}ç§’")
    
    return result

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” æ­£åœ¨åŠ è½½ä¼˜åŒ–ç‰ˆ RAG çŸ¥è¯†åº“...")
    
    try:
        db = load_vectorstore()
        print("âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸï¼")
        
        qa_chain = create_qa_chain(db)
        print("âœ… ä¼˜åŒ–ç‰ˆé—®ç­”ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
        print("ğŸš€ ç‰¹æ€§ï¼šç¼“å­˜ç¿»è¯‘ã€å¿«é€Ÿæ¨¡å‹ã€æ€§èƒ½ç›‘æ§")
        
        print("\n" + "="*60)
        print("ğŸ§  å¿ƒç†å¥åº·ä¼˜åŒ–ç‰ˆ RAG é—®ç­”ç³»ç»Ÿ")
        print("ğŸ’¬ æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡æé—®ï¼ˆå·²ä¼˜åŒ–å»¶è¿Ÿï¼‰")
        print("è¾“å…¥ 'quit' æˆ– 'exit' æˆ– 'é€€å‡º' ç»“æŸ")
        print("è¾“å…¥ 'cache' æŸ¥çœ‹ç¼“å­˜ç»Ÿè®¡")
        print("="*60)
        
        while True:
            question = input("\nâ“ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
            
            if question.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                print("ğŸ‘‹ å†è§ï¼")
                break
            
            if question.lower() == 'cache':
                print(f"ğŸ“Š ç¼“å­˜æ¡ç›®æ•°: {len(translation_cache.cache)}")
                continue
                
            if not question:
                print("è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜ã€‚")
                continue
            
            try:
                ask_question_optimized(qa_chain, question)
            except Exception as e:
                print(f"âŒ å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {e}")
                
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")

if __name__ == "__main__":
    main()
