# rebuild_vector_db.py
# é‡æ–°æ„å»ºå‘é‡æ•°æ®åº“ - ä¼˜åŒ–ç‰ˆæœ¬

import os
import sys
from dotenv import load_dotenv

def main():
    print("ğŸ”„ é‡æ–°æ„å»ºå‘é‡æ•°æ®åº“...")
    print("="*50)

    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # æ£€æŸ¥ API Key
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âŒ æœªæ‰¾åˆ° OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        print("ğŸ’¡ è¯·ç¡®ä¿ .env æ–‡ä»¶ä¸­è®¾ç½®äº† OPENAI_API_KEY")
        return False

    try:
        # å°è¯•ä¸åŒçš„å¯¼å…¥æ–¹å¼
        print("ğŸ“¦ å¯¼å…¥ä¾èµ–åŒ…...")
        
        # å¯¼å…¥åŸºç¡€åŒ…
        from langchain_community.document_loaders import TextLoader
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        from langchain_community.vectorstores import FAISS
        
        # å°è¯•å¯¼å…¥ OpenAI Embeddings
        embeddings = None
        try:
            from langchain_openai import OpenAIEmbeddings
            print("âœ… ä½¿ç”¨ langchain_openai")
            embeddings = OpenAIEmbeddings(
                openai_api_key=api_key,
                model="text-embedding-ada-002"
            )
        except ImportError as e1:
            print(f"âš ï¸ langchain_openai å¤±è´¥: {e1}")
            try:
                from langchain.embeddings.openai import OpenAIEmbeddings
                print("âœ… ä½¿ç”¨ langchain.embeddings.openai")
                embeddings = OpenAIEmbeddings(openai_api_key=api_key)
            except ImportError as e2:
                print(f"âŒ ä¸¤ç§å¯¼å…¥æ–¹å¼éƒ½å¤±è´¥:")
                print(f"   æ–°ç‰ˆæœ¬: {e1}")
                print(f"   æ—§ç‰ˆæœ¬: {e2}")
                return False
        except Exception as e:
            print(f"âŒ Embeddings åˆå§‹åŒ–å¤±è´¥: {e}")
            # å°è¯•ä¸æŒ‡å®šæ¨¡å‹
            try:
                embeddings = OpenAIEmbeddings(openai_api_key=api_key)
                print("âœ… ä½¿ç”¨é»˜è®¤æ¨¡å‹åˆ›å»º Embeddings")
            except Exception as e2:
                print(f"âŒ é»˜è®¤æ–¹å¼ä¹Ÿå¤±è´¥: {e2}")
                return False

        if not embeddings:
            print("âŒ æ— æ³•åˆ›å»º OpenAI Embeddings")
            return False

        # è®¾ç½®è·¯å¾„
        DOCS_PATH = "docs"
        VECTOR_DB_PATH = "vector_store"

        print(f"ğŸ“ æ–‡æ¡£è·¯å¾„: {DOCS_PATH}")
        print(f"ğŸ“ å‘é‡æ•°æ®åº“è·¯å¾„: {VECTOR_DB_PATH}")

        # æ£€æŸ¥æ–‡æ¡£ç›®å½•
        if not os.path.exists(DOCS_PATH):
            print(f"âŒ æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {DOCS_PATH}")
            return False

        # 1. åŠ è½½æ‰€æœ‰æ–‡æœ¬æ–‡æ¡£
        print("\nğŸ“š åŠ è½½æ–‡æ¡£...")
        all_docs = []
        doc_count = 0
        
        txt_files = [f for f in os.listdir(DOCS_PATH) if f.endswith(".txt")]
        if not txt_files:
            print(f"âŒ åœ¨ {DOCS_PATH} ç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½• .txt æ–‡ä»¶")
            return False
        
        for file_name in txt_files:
            file_path = os.path.join(DOCS_PATH, file_name)
            try:
                loader = TextLoader(file_path, encoding="utf-8")
                docs = loader.load()
                all_docs.extend(docs)
                doc_count += 1
                print(f"  âœ… {file_name} ({len(docs)} ä¸ªæ–‡æ¡£å¯¹è±¡)")
            except Exception as e:
                print(f"  âŒ {file_name}: {e}")
                # å°è¯•å…¶ä»–ç¼–ç 
                try:
                    loader = TextLoader(file_path, encoding="latin-1")
                    docs = loader.load()
                    all_docs.extend(docs)
                    doc_count += 1
                    print(f"  âœ… {file_name} (ä½¿ç”¨ latin-1 ç¼–ç )")
                except Exception as e2:
                    print(f"  âŒ {file_name} å®Œå…¨å¤±è´¥: {e2}")

        print(f"ğŸ“– æ€»å…±åŠ è½½äº† {doc_count} ä¸ªæ–‡ä»¶ï¼Œ{len(all_docs)} ä¸ªæ–‡æ¡£å¯¹è±¡")

        if not all_docs:
            print("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ–‡æ¡£")
            return False

        # 2. åˆ†å‰²æ–‡æ¡£
        print("\nâœ‚ï¸ åˆ†å‰²æ–‡æ¡£...")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500, 
            chunk_overlap=50,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
        docs_split = text_splitter.split_documents(all_docs)
        print(f"ğŸ“ åˆ†å‰²åå¾—åˆ° {len(docs_split)} ä¸ªæ–‡æ¡£å—")

        if not docs_split:
            print("âŒ æ–‡æ¡£åˆ†å‰²å¤±è´¥")
            return False

        # 3. æµ‹è¯•åµŒå…¥åŠŸèƒ½
        print("\nğŸ§ª æµ‹è¯• Embeddings...")
        try:
            test_embedding = embeddings.embed_query("test")
            print(f"âœ… Embeddings æµ‹è¯•æˆåŠŸï¼Œå‘é‡ç»´åº¦: {len(test_embedding)}")
        except Exception as e:
            print(f"âŒ Embeddings æµ‹è¯•å¤±è´¥: {e}")
            return False

        # 4. æ„å»ºå‘é‡æ•°æ®åº“
        print("\nğŸ”¨ æ„å»ºå‘é‡æ•°æ®åº“...")
        try:
            print("   æ­£åœ¨åˆ›å»ºå‘é‡æ•°æ®åº“ï¼Œè¯·ç¨å€™...")
            db = FAISS.from_documents(docs_split, embeddings)
            print("âœ… å‘é‡æ•°æ®åº“æ„å»ºæˆåŠŸ")
            
            # æµ‹è¯•æ•°æ®åº“
            print("\nğŸ” æµ‹è¯•å‘é‡æ•°æ®åº“...")
            test_queries = ["attachment theory", "trauma", "therapy"]
            for query in test_queries:
                try:
                    results = db.similarity_search(query, k=2)
                    print(f"  âœ… '{query}': æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
                except Exception as e:
                    print(f"  âŒ '{query}': {e}")
            
        except Exception as e:
            print(f"âŒ å‘é‡æ•°æ®åº“æ„å»ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

        # 5. ä¿å­˜å‘é‡æ•°æ®åº“
        print("\nğŸ’¾ ä¿å­˜å‘é‡æ•°æ®åº“...")
        try:
            # å¦‚æœç›®å½•å­˜åœ¨ï¼Œå…ˆåˆ é™¤
            if os.path.exists(VECTOR_DB_PATH):
                import shutil
                shutil.rmtree(VECTOR_DB_PATH)
                print("ğŸ—‘ï¸ åˆ é™¤æ—§çš„å‘é‡æ•°æ®åº“")
            
            # åˆ›å»ºç›®å½•
            os.makedirs(VECTOR_DB_PATH, exist_ok=True)
            
            # ä¿å­˜æ•°æ®åº“
            db.save_local(VECTOR_DB_PATH)
            print("âœ… å‘é‡æ•°æ®åº“ä¿å­˜æˆåŠŸ")
            
            # éªŒè¯ä¿å­˜çš„æ–‡ä»¶
            saved_files = os.listdir(VECTOR_DB_PATH)
            print(f"ğŸ“ ä¿å­˜çš„æ–‡ä»¶: {saved_files}")
            
            # éªŒè¯å¯ä»¥é‡æ–°åŠ è½½
            print("\nğŸ”„ éªŒè¯æ•°æ®åº“åŠ è½½...")
            test_db = FAISS.load_local(VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)
            test_result = test_db.similarity_search("test", k=1)
            print(f"âœ… æ•°æ®åº“éªŒè¯æˆåŠŸï¼Œå¯ä»¥æ­£å¸¸åŠ è½½å’ŒæŸ¥è¯¢")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

        print("\n" + "="*50)
        print("ğŸ‰ å‘é‡æ•°æ®åº“é‡å»ºå®Œæˆï¼")
        print("âœ… ç°åœ¨å¯ä»¥è¿è¡ŒæŸ¥è¯¢ç³»ç»Ÿäº†")
        print("ğŸ’¡ è¿è¡Œå‘½ä»¤: python query_rag.py")
        return True
        
    except Exception as e:
        print(f"âŒ é‡å»ºè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    if not success:
        print("\nâŒ å‘é‡æ•°æ®åº“é‡å»ºå¤±è´¥")
        sys.exit(1)
    else:
        print("\nğŸŠ æ‰€æœ‰æ“ä½œå®Œæˆï¼")
